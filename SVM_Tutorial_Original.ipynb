{
 "metadata": {
  "name": "",
  "signature": "sha256:d49f4fa4119219948564ba99cbe85b15cf13dc003cb799f6b902ad60bd325494"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook is based on a tutotial given by [Jake Vanderplas](http://www.vanderplas.com) for PyCon 2014. Source and license info is on [GitHub](https://github.com/jakevdp/sklearn_pycon2014/).</i></small>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Support Vector Machines (SVMs) are a powerful supervised learning algorithm used for **classification** or for **regression**. SVMs are a **discriminative** classifier: that is, they draw a boundary between clusters of data.\n",
      "\n",
      "Let's show a quick example of support vector classification. First we need to create a dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets.samples_generator import make_blobs\n",
      "X, y = make_blobs(n_samples=50, centers=2,\n",
      "                  random_state=0, cluster_std=0.60)\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y, s=50);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we'll fit a Support Vector Machine Classifier to these points:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
      "clf = SVC(kernel='linear')\n",
      "clf.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To better visualize what's happening here, let's create a quick convenience function that will plot SVM decision boundaries for us:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_svc_decision_function(clf):\n",
      "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
      "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
      "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
      "    Y, X = np.meshgrid(y, x)\n",
      "    P = np.zeros_like(X)\n",
      "    for i, xi in enumerate(x):\n",
      "        for j, yj in enumerate(y):\n",
      "            P[i, j] = clf.decision_function([xi, yj])\n",
      "    return plt.contour(X, Y, P, colors='k',\n",
      "                       levels=[-1, 0, 1],\n",
      "                       linestyles=['--', '-', '--'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n",
      "plot_svc_decision_function(clf);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that the dashed lines touch a couple of the points: these points are known as the \"support vectors\", and are stored in the ``support_vectors_`` attribute of the classifier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n",
      "plot_svc_decision_function(clf)\n",
      "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
      "            s=200, facecolors='none');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The unique thing about SVM is that only the support vectors matter: that is, if you moved any of the other points without letting them cross the decision boundaries, they would have no effect on the classification results!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = SVC(kernel='rbf')\n",
      "clf.fit(X, y)\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n",
      "plot_svc_decision_function(clf)\n",
      "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
      "            s=200, facecolors='none');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Digits Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we'll take a look at another dataset, one where we have to put a bit more thought into how to represent the data. We can explore the data in a similar manner as above:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "digits = load_digits()\n",
      "digits.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = digits.data\n",
      "y = digits.target\n",
      "print(X.shape)\n",
      "print(y.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print digits.data[0]\n",
      "print digits.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The target here is just the digit represented by the data. The data is an array of length 64... but what does this data mean?\n",
      "\n",
      "There's a clue in the fact that we have two versions of the data array: data and images. Let's take a look at them:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print digits.data.shape\n",
      "print digits.images.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that they're related by a simple reshaping:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.all(digits.images.reshape((1797, 64)) == digits.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's visualize the data. It's little bit more involved than the simple scatter-plot we used above, but we can do it rather tersely."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set up the figure\n",
      "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
      "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
      "\n",
      "# plot the digits: each image is 8x8 pixels\n",
      "for i in range(64):\n",
      "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
      "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
      "    \n",
      "    # label the image with the target value\n",
      "    ax.text(0, 7, str(digits.target[i]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see now what the features mean. Each feature is a real-valued quantity representing the darkness of a pixel in an 8x8 image of a hand-written digit.\n",
      "\n",
      "Even though each sample has data that is inherently two-dimensional, the data matrix flattens this 2D data into a single vector, which can be contained in one row of the data matrix."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise: Classifying Digits"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's classify the digits using a Support Vector Classifier and try out 2 different kernels to see which one performs better."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import metrics\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n",
      "\n",
      "for kernel in ['rbf', 'linear']:\n",
      "    clf = SVC(kernel=kernel).fit(Xtrain, ytrain)\n",
      "    ypred = clf.predict(Xtest)\n",
      "    print(\"SVC: kernel = {0}\".format(kernel))\n",
      "    print(metrics.f1_score(ytest, ypred))\n",
      "    plt.figure()\n",
      "    plt.imshow(metrics.confusion_matrix(ypred, ytest),\n",
      "               interpolation='nearest', cmap=plt.cm.binary)\n",
      "    plt.colorbar()\n",
      "    plt.xlabel(\"true label\")\n",
      "    plt.ylabel(\"predicted label\")\n",
      "    plt.title(\"SVC: kernel = {0}\".format(kernel))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. The higher the diagonal values of the confusion matrix the better, indicating many correct predictions."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}