{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest Neighbor Tutorial\n",
    "===\n",
    "\n",
    "K-nearest neighbors, or K-NN, is a simple form of supervised learning. It assigns an output label to a new input example x based on it's closest neighboring datapoints. The number K is the number of data points to use. For K=1, x is assigned the label of the closest neighbor. If K>1, the majority vote is used to label x.\n",
    "\n",
    "The code in this tutorial is slightly modified from the scikit-learn [K-NN example](http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#example-neighbors-plot-classification-py). There is also information on the K-NN classifier function [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "===\n",
    "Tell matplotlib to print figures in the notebook. Then import numpy (for numerical data), matplotlib.pyplot (for plotting figures), ListedColormap (for plotting colors), neighbors (for the scikit-learn nearest-neighbor algorithm), and datasets (to download the iris dataset from scikit-learn).\n",
    "\n",
    "Also create the color maps to use to color the plotted data, and \"labelList\", which is a list of colored rectangles to use in plotted legends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print figures in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets # Import the nerest neighbor function and dataset from scikit-learn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Import patch for drawing rectangles in the legend\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Create color maps\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "\n",
    "# Create a legend for the colors, using rectangles for the corresponding colormap colors\n",
    "labelList = []\n",
    "for color in cmap_bold.colors:\n",
    "    labelList.append(Rectangle((0, 0), 1, 1, fc=color))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset\n",
    "===\n",
    "Import the dataset and store it to a variable called iris. Scikit-learn's explanation of the dataset is [here](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html). This dataset is similar to a python dictionary, with the keys: ['DESCR', 'target_names', 'target', 'data', 'feature_names']\n",
    "\n",
    "The data features are stored in iris.data, where each row is an example from a single flower, and each column is a single feature. The feature names are stored in iris.feature_names. Labels are stored as the numbers 0, 1, or 2 in iris.target, and the names of these labels are in iris.target_names.\n",
    "\n",
    "The dataset consists of measurements made on 50 examples from each of three different species of iris flowers (Setosa, Versicolour, and Virginica). Each example has four features (or measurements): [sepal](http://en.wikipedia.org/wiki/Sepal) length, sepal width, [petal](http://en.wikipedia.org/wiki/Petal) length, and petal width. All measurements are in cm.\n",
    "\n",
    "Below, we load the labels into y, the corresponding label names into labelNames, the data into X, and the names of the features into featureNames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Store the labels (y), label names, features (X), and feature names\n",
    "y = iris.target       # Labels are stored in y as numbers\n",
    "labelNames = iris.target_names # Species names corresponding to labels 0, 1, and 2\n",
    "X = iris.data\n",
    "featureNames = iris.feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we plot the first two features from the dataset (sepal length and width). Normally we would try to use all useful features, but sticking with two allows us to visualize the data more easily.\n",
    "\n",
    "Then we plot the data to get a look at what we're dealing with. The colormap is used to determine what colors are used for each class when plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "\n",
    "# Sepal length and width\n",
    "X_small = X[:,:2]\n",
    "# Get the minimum and maximum values with an additional 0.5 border\n",
    "x_min, x_max = X_small[:, 0].min() - .5, X_small[:, 0].max() + .5\n",
    "y_min, y_max = X_small[:, 1].min() - .5, X_small[:, 1].max() + .5\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_small[:, 0], X_small[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlabel('Sepal length (cm)')\n",
    "plt.ylabel('Sepal width (cm)')\n",
    "plt.title('Sepal width vs length')\n",
    "\n",
    "# Set the plot limits\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "# Plot the legend\n",
    "plt.legend(labelList, labelNames)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest neighbors: training\n",
    "===\n",
    "Next, we train a nearest neighbor classifier on our data. \n",
    "\n",
    "The first section chooses the number of neighbors to use, and stores it in the variable n_neighbors (another, more intuitive, name for the K variable mentioned previously). \n",
    "\n",
    "The last two lines create and train the classifier. Line 1 creates a classifier (clf) using the [KNeighborsClassifier()](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) function, and tells it to use the number of neighbors stored in n_neighbors. Line 2 uses the fit() method to train the classifier on the features in X_small, using the labels in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your number of neighbors\n",
    "n_neighbors = 15\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(X_small, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the classification boundaries\n",
    "===\n",
    "Now that we have our classifier, let's visualize what it's doing. \n",
    "\n",
    "First we plot the decision boundaries, or the lines dividing areas assigned to the different labels (species of iris). Then we plot our examples onto the space, showing where each point lies and the corresponding decision boundary.\n",
    "\n",
    "The colored background shows the areas that are considered to belong to a certain label. If we took sepal measurements from a new flower, we could plot it in this space and use the color to determine which type of iris our classifier believes it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "x_min, x_max = X_small[:, 0].min() - 1, X_small[:, 0].max() + 1\n",
    "y_min, y_max = X_small[:, 1].min() - 1, X_small[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # Make a prediction oat every point \n",
    "                                               # in the mesh in order to find the \n",
    "                                               # classification areas for each label\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_small[:, 0], X_small[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i)\"\n",
    "         % (n_neighbors))\n",
    "plt.xlabel('Sepal length (cm)')\n",
    "plt.ylabel('Sepal width (cm)')\n",
    "\n",
    "# Plot the legend\n",
    "plt.legend(labelList, labelNames)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the number of neighbors\n",
    "===\n",
    "\n",
    "Change the number of neighbors (n_neighbors) below, and see how the plot boundaries change. Try as many or as few as you'd like, but remember there are only 150 examples in the dataset, so selecting all 150 wouldn't be very useful!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your number of neighbors\n",
    "n_neighbors = # Choose a new number of neighbors\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(X_small, y)\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "x_min, x_max = X_small[:, 0].min() - 1, X_small[:, 0].max() + 1\n",
    "y_min, y_max = X_small[:, 1].min() - 1, X_small[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # Make a prediction oat every point \n",
    "                                               # in the mesh in order to find the \n",
    "                                               # classification areas for each label\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_small[:, 0], X_small[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i)\"\n",
    "         % (n_neighbors))\n",
    "plt.xlabel('Sepal length (cm)')\n",
    "plt.ylabel('Sepal width (cm)')\n",
    "\n",
    "# Plot the legend\n",
    "plt.legend(labelList, labelNames)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions\n",
    "===\n",
    "\n",
    "Now, let's say we go out and measure the sepals of two new iris plants, and want to know what species they are. We're going to use our classifier to predict the flowers with the following measurements:\n",
    "\n",
    "Plant | Sepal length | Sepal width\n",
    "------|--------------|------------\n",
    "A     |4.3           |2.5\n",
    "B     |6.3           |2.1\n",
    "\n",
    "We can use our classifier's [predict()](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.predict) function to predict the label for our input features. We pass in the variable examples to the predict() function, which is a list, and each element is another list containing the features (measurements) for a particular example. The output is a list of labels corresponding to the input examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our new data examples\n",
    "examples = [[4.3, 2.5], # Plant A\n",
    "            [6.3, 2.1]] # Plant B\n",
    "\n",
    "# Reset our number of neighbors\n",
    "n_neighbors = 15\n",
    "\n",
    "# Create an instance of Neighbours Classifier and fit the original data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(X_small, y)\n",
    "\n",
    "# Predict the labels for our new examples\n",
    "labels = clf.predict(examples)\n",
    "\n",
    "# Print the predicted species names\n",
    "print('A: ' + labelNames[labels[0]])\n",
    "print('B: ' + labelNames[labels[1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting our predictions\n",
    "---\n",
    "Now let's plot our predictions to see why they were classified that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the results\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "x_min, x_max = X_small[:, 0].min() - 1, X_small[:, 0].max() + 1\n",
    "y_min, y_max = X_small[:, 1].min() - 1, X_small[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # Make a prediction oat every point \n",
    "                                               # in the mesh in order to find the \n",
    "                                               # classification areas for each label\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_small[:, 0], X_small[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i)\"\n",
    "         % (n_neighbors))\n",
    "plt.xlabel('Sepal length (cm)')\n",
    "plt.ylabel('Sepal width (cm)')\n",
    "\n",
    "# Display the new examples as labeled text on the graph\n",
    "plt.text(examples[0][0], examples[0][1],'A', fontsize=14)\n",
    "plt.text(examples[1][0], examples[1][1],'B', fontsize=14)\n",
    "\n",
    "# Plot the legend\n",
    "plt.legend(labelList, labelNames)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about our other features?\n",
    "===\n",
    "You may remember that our original dataset contains two additional features, the length and width of the petals.\n",
    "\n",
    "What does the plot look like when you train on the petal length and width? How does it change when you change the number of neighbors?\n",
    "\n",
    "How would you plot our two new plants, A and B, on these new plots? Assume we have all four measurements for each plant, as shown below.\n",
    "\n",
    "Plant | Sepal length | Sepal width| Petal length | Petal width\n",
    "------|--------------|------------|--------------|------------\n",
    "A     |4.3           |2.5         | 1.5          | 0.5\n",
    "B     |6.3           |2.1         | 4.8          | 1.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your code here! \n",
    "\n",
    "# Feel free to add as many code cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing neighbor weights\n",
    "===\n",
    "Looking at our orignal plot of sepal dimensions, we can see that plant A is classified as Setosa (red) and B is classified as Virginica (blue). While A seems to be clearly correct, B is much closer to two examples from Versicolour (green). Maybe we should give more importance to labels that are closer to our example?\n",
    "\n",
    "In the previous examples, all the neighbors were considered equally important when deciding what label to give our input. But what if we want to give more importance (or a heigher weight) to neighbors that are closer to our new example? The K-NN algorithm allows you to change from uniform weights, where all included neighbors have the same importance, to distance-based weights, where closer neighbors are given more consideration. \n",
    "\n",
    "Below, we create a new classifier using distance-based weights and plot the results. The only difference in the code is that we call KNeighborsClassifier() with the argument weights='distance'. \n",
    "\n",
    "Look at how it's different from the original plot, and see how the classifications of plant B change. Try it with different combinations of neighbors, and compare it to the previous plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your number of neighbors\n",
    "n_neighbors_distance = 15\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf_distance = neighbors.KNeighborsClassifier(n_neighbors_distance, \n",
    "                                              weights='distance')\n",
    "clf_distance.fit(X_small, y)\n",
    "\n",
    "# Predict the labels of the new examples\n",
    "labels = clf_distance.predict(examples)\n",
    "\n",
    "# Print the predicted species names\n",
    "print('A: ' + labelNames[labels[0]])\n",
    "print('B: ' + labelNames[labels[1]])\n",
    "\n",
    "# Plot the results\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "x_min, x_max = X_small[:, 0].min() - 1, X_small[:, 0].max() + 1\n",
    "y_min, y_max = X_small[:, 1].min() - 1, X_small[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = clf_distance.predict(np.c_[xx.ravel(), yy.ravel()]) # Make a prediction oat every point \n",
    "                                               # in the mesh in order to find the \n",
    "                                               # classification areas for each label\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_small[:, 0], X_small[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i)\"\n",
    "         % (n_neighbors))\n",
    "plt.xlabel('Sepal length (cm)')\n",
    "plt.ylabel('Sepal width (cm)')\n",
    "\n",
    "# Display the new examples as labeled text on the graph\n",
    "plt.text(examples[0][0], examples[0][1],'A', fontsize=14)\n",
    "plt.text(examples[1][0], examples[1][1],'B', fontsize=14)\n",
    "\n",
    "# Plot the legend\n",
    "plt.legend(labelList, labelNames)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, see how this affects your plots using other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using more than two features\n",
    "===\n",
    "Using two features is great for visualizing, but it's often not good for training a good classifier. Below, we will train a classifier using the 'distance' weighting method and all four features, and use that to predict plants A and B.\n",
    "\n",
    "How do the predictions compare to our predictions using only two labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our new data examples\n",
    "examples = [[4.3, 2.5, 1.5, 0.5], # Plant A\n",
    "            [6.3, 2.1, 4.8, 1.5]] # Plant B\n",
    "\n",
    "# Choose your number of neighbors\n",
    "n_neighbors_distance = 15\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf_distance = neighbors.KNeighborsClassifier(n_neighbors_distance, weights='distance')\n",
    "clf_distance.fit(X, y)\n",
    "\n",
    "# Predict the labels of the new examples\n",
    "labels = clf_distance.predict(examples)\n",
    "\n",
    "# Print the predicted species names\n",
    "print('A: ' + labelNames[labels[0]])\n",
    "print('B: ' + labelNames[labels[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating The Classifier\n",
    "===\n",
    "\n",
    "In order to evaluate a classifier, we need to split our dataset into training data, which we'll show to the classifier so it can learn, and testing data, which we will hold back from training and use to test its predictions.\n",
    "\n",
    "Below, we create the training and testing datasets, using all four features. We then train our classifier on the training data, and get the predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Choose your number of neighbors\n",
    "n_neighbors_distance = 15\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf_distance = neighbors.KNeighborsClassifier(n_neighbors_distance, weights='distance')\n",
    "clf_distance.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data\n",
    "predictions = clf_distance.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we evaluate how well the classifier did. The easiest way to do this is to get the average number of correct predictions, usually referred to as the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = np.mean(predictions == y_test )*100\n",
    "\n",
    "print('The accuracy is ' + '%.2f' % accuracy + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comparing Models with Crossvalidation\n",
    "===\n",
    "\n",
    "To select the best number of neighbors to use in our model, we need to use crossvalidation. We can then get our final result using our test data.\n",
    "\n",
    "First we choose the number of neighbors we want to investigate, then divide our training data into folds. We loop through the sets of training and validation folds. Each time, we train each model on the training data and evaluate on the validation data. We store the accuracy of each classifier on each fold so we can look at them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose our k values\n",
    "kvals = [1,3,5,10,20,40]\n",
    "\n",
    "# Create a dictionary of arrays to store accuracies\n",
    "accuracies = {}\n",
    "for k in kvals:\n",
    "    accuracies[k] = []\n",
    "\n",
    "# Loop through 5 folds\n",
    "kf = KFold(n_splits=5)\n",
    "for trainInd, valInd in kf.split(X_train):\n",
    "    X_tr = X_train[trainInd,:]\n",
    "    y_tr = y_train[trainInd]\n",
    "    X_val = X_train[valInd,:]\n",
    "    y_val = y_train[valInd]\n",
    "    \n",
    "    # Loop through each value of k\n",
    "    for k in kvals:\n",
    "        \n",
    "        # Create the classifier\n",
    "        clf = neighbors.KNeighborsClassifier(k, weights='distance')\n",
    "        \n",
    "        # Train the classifier\n",
    "        clf.fit(X_tr, y_tr) \n",
    "        \n",
    "        # Make our predictions\n",
    "        pred = clf.predict(X_val)\n",
    "    \n",
    "        # Calculate the accuracy\n",
    "        accuracies[k].append(np.mean(pred == y_val))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a Model\n",
    "---\n",
    "\n",
    "To select a model, we look at the average accuracy across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in kvals:\n",
    "    print('k=%i: %.2f' % (k, np.mean(accuracies[k])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Evaluation\n",
    "---\n",
    "\n",
    "K=3 gives us the highest accuracy, so we select it as our best model. Now we can evaluate it on our training set and get our final accuracy rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = neighbors.KNeighborsClassifier(3, weights='distance')\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print('The final accuracy is ' + '%.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sidenote: Randomness and Results\n",
    "===\n",
    "\n",
    "Every time you run this notebook, you will get slightly different results. Why? Because data is randomly divided among the training/testing/validation data sets. Running the code again will create a different division of the data, and will make the results slightly different. However, the overall outcome should remain consistent and have approximately the same values. If you have drastically different results when running an analysis multiple times, it suggests a problem with your model or that you need more data.\n",
    "\n",
    "If it's important that you get the exact same results every time you run the code, you can specify a random state in the `random_state` argument of `train_test_split()` and `KFold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
